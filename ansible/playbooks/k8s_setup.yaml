- hosts: all
  become: true  # root yetkisi veriyor. Asagidaki islemleri root yetkisi ile yapiyoruz.
  tasks:

  - name: change hostnames
    shell: "hostnamectl set-hostname {{ hostvars[inventory_hostname]['private_dns_name'] }}"
    # hostvars default variable lardan birisi. Belirttigimiz host lari hostvars altinda tanimliyor
    # bu sekilde hostname lerini dynamic bir sekilde ayarlamis oluyorum. Örnegimizde 3 nodes var.

  - name: swap off    
    shell: |
      free -m
      swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab
    # AWS EC2 larda swap alani yok. Normalde kullanicilar tüm verileri RAM den aliyor. RAM gecici bellek. Bilgisayar kapaninca buradaki bilgiler gidiyor. 3 cesit bellek var: cache (islemcinin ön bellegi ve en hizli), ram (hizli) ve hard disk (yavas). Hard disk yavas calisir. Buradan bilgiler daha hizli olan ram e aktarilir. Ama ram sinirlidir ve hard diske göre kücüktür (örnegin 16 gb gibi). 
    # ram dolunca, ram ram deki bazi verileri ram formatinda swap alanina aktariyor. Tekrar kullanma durumumuz olursa; ram bu verileri swap ten alir ve dogrudan kullanir. swap gecici ram depolama alani diyebiliriz. swap bilgisayarin performansini azaltmaktadir.
    # swap icin ayrilan bölüme erisme sansimiz yok.
    # AWS instance larda swap alani yok.
    # kubernetes swap kullanimi istemiyor, performans düsüklügü yasamamak icin
    # performans düsmesini engellemek icin bunu yaptik

  - name: Enable the nodes to see bridged traffic
    shell: |
      cat << EOF | sudo tee /etc/sysctl.d/k8s.conf
      net.bridge.bridge-nf-call-ip6tables = 1
      net.bridge.bridge-nf-call-iptables = 1
      EOF
      sysctl --system

  - name: update apt-get
    shell: apt-get update

  - name: Install packages that allow apt to be used over HTTPS
    apt:
      name: "{{ packages }}"
      state: present
      update_cache: yes
    vars:
      packages:
      - apt-transport-https  
      - curl
      - ca-certificates

  - name: update apt-get and install kube packages
    shell: |
      curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - && \
      echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list && \
      apt-get update -q && \
      apt-get install -qy kubelet=1.23.5-00 kubectl=1.23.5-00 kubeadm=1.23.5-00 docker.io
    # gpg bir token. Güvenlik nedeniyle gpg yi indiriyoruz.

  # ubuntu kullanicisi olarak login oluyoruz. bu nedenle ubuntu user ini docker group a ekliyoruz.
  - name: Add ubuntu to docker group
    user:
      name: ubuntu
      group: docker

  - name: Restart docker and enable
    service:
      name: docker
      state: restarted
      enabled: yes

  # change the Docker cgroup driver by creating a configuration file `/etc/docker/daemon.json` 
  # and adding the following line then restart deamon, docker and kubelet

  - name: change the Docker cgroup
    shell: |
      echo '{"exec-opts": ["native.cgroupdriver=systemd"]}' | sudo tee /etc/docker/daemon.json
      sudo systemctl daemon-reload
      sudo systemctl restart docker
      sudo systemctl restart kubelet


# burada master node ile ilgili config ayarlarini yapiyoruz
- hosts: role_master
  tasks:
      
  - name: pull kubernetes images before installation
    become: yes
    shell: kubeadm config images pull

  - name: copy the configuration
    become: yes
    copy: 
      src: ./clusterconfig-base.yml
      dest: /home/ubuntu/
  
  # gettext-base tool ü yükleniyor
  - name: get gettext-base
    become: true
    apt:
      package: gettext-base
      state: present

  # buradaki hostvars "role_master" host undan geliyor.
  # "envsubst" komutu ile place holder yerine gercek degerini yazdir ve bu haliyle yeni bir file olustur diyoruz. Bu sekilde template olan file degismeden yeni bir file icerisinde environment variable larin gercek degerini yazdirmis oluyoruz.
  - name: change controlplane_endpoint and produce the clusterconfig.yml file
    shell: |
      export CONTROLPLANE_ENDPOINT={{ hostvars[inventory_hostname]['private_ip_address'] }}
      envsubst < /home/ubuntu/clusterconfig-base.yml > /home/ubuntu/clusterconfig.yml

  # kubeadm i burada initialize ediyoruz
  - name: initialize the Kubernetes cluster using kubeadm
    become: true
    shell: |
      kubeadm init --config /home/ubuntu/clusterconfig.yml
    
  - name: Setup kubeconfig for ubuntu user
    become: true
    command: "{{ item }}"
    with_items:
     - mkdir -p /home/ubuntu/.kube
     - cp -i /etc/kubernetes/admin.conf /home/ubuntu/.kube/config
     - chown ubuntu:ubuntu /home/ubuntu/.kube/config  # bu sekilde file in owner ligini ubuntu user ina verdik. Artik sudo ile root yetkisinde kubectl yapsak bile calistiramayiz.

  - name: Install flannel pod network
    shell: kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml

  - name: Generate join command
    become: true
    command: kubeadm token create --print-join-command
    register: join_command_for_workers

  - debug: msg='{{ join_command_for_workers.stdout.strip() }}'

  - name: register join command for workers
    add_host: # host üretiyoruz/ekliyoruz. Ismi de "kube_master". Bu normal olusturdugumuz master node dan farkli bir node. Bizim master node umuzun ismi kube-master. Burada olusturdugumuz host un ismi kube_master. Bu ismi baska sekilde de verebilirdik.
      name: "kube_master"
      worker_join: "{{ join_command_for_workers.stdout.strip() }}"  # burada da olusturdugumuz bu host a bir attribute atadik
  # join token ini master node da olusturduk. worker node da join token i calistirmamiz gerekiyor. Bu maksatla: 1. token i S3 at oradan cek. 2. baska bir host olusturup buna worker_join seklinde attribute atiyoruz. Bu bir linux komutu oldugu icin worker node da bu attribute u cagirinca linux komutu calismis olacak. Bunu yapmamizin sebebi; bir play den baska bir play e deger atayamiyorum. Bu sorunu asmak icin host olusturmus oluyoruz. play ler arasi host lar kullanilabiliyor.
  # "stdout" ekran ciktisi yani ekrana yazilan sonuc/cikti demektir.

- hosts: role_worker
  become: true
  tasks:

  # join command i calistirmis oluyoruz. Bu sayede worker node cluster a dahil edilmis oluyor.
  - name: Join workers to cluster
    shell: "{{ hostvars['kube_master']['worker_join'] }}"
    register: result_of_joining

- hosts: role_master
  become: false
  tasks:

  # K8s de "patch" komutu; yaml file larini edit etmeye modify etmeye yariyor.
  - name: Patch the instances
    become: false
    shell: |
      cd /home/ubuntu
      kubectl patch node {{ hostvars[groups['role_master'][0]]['private_dns_name'] }} -p '{"spec":{"providerID":"aws:///us-east-1a/{{ hostvars[groups['role_master'][0]]['instance_id'] }}" }}'
      kubectl patch node {{ hostvars[groups['role_worker'][0]]['private_dns_name'] }} -p '{"spec":{"providerID":"aws:///us-east-1a/{{ hostvars[groups['role_worker'][0]]['instance_id'] }}" }}'
      kubectl patch node {{ hostvars[groups['role_worker'][1]]['private_dns_name'] }} -p '{"spec":{"providerID":"aws:///us-east-1a/{{ hostvars[groups['role_worker'][1]]['instance_id'] }}" }}'

  # cloud control manager i helm ile install ediyoruz
  - name: Deploy the required cloud-controller-manager 
    shell: |
      cd /home/ubuntu
      curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
      chmod 777 get_helm.sh
      ./get_helm.sh
      helm repo add aws-cloud-controller-manager https://kubernetes.github.io/cloud-provider-aws
      helm repo update
      helm upgrade --install aws-cloud-controller-manager aws-cloud-controller-manager/aws-cloud-controller-manager --set image.tag=v1.20.0-alpha.0

  # burada Nginx Ingress i install ediyoruz    
  - name: Deploy Nginx Ingress 
    shell: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.2/deploy/static/provider/aws/deploy.yaml